{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dac681e-6794-412c-bbeb-9cdf42c88578",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'dataset/my-own-dataset' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, precision_score, recall_score, classification_report\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 加载数据集\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/my-own-dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#needed to construct a new Dataset class\u001b[39;00m\n\u001b[0;32m      9\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 分词器和模型\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\datasets\\load.py:2548\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2543\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2544\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2545\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2548\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2549\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2550\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2551\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2552\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2553\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2554\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2555\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2556\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2557\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2558\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2559\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2560\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2561\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2563\u001b[0m )\n\u001b[0;32m   2565\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\datasets\\load.py:2220\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2218\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   2219\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 2220\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\datasets\\load.py:1865\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1868\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1869\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1870\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\datasets\\load.py:1812\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1810\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1811\u001b[0m     msg \u001b[38;5;241m=\u001b[39m msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at revision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;28;01melse\u001b[39;00m msg\n\u001b[1;32m-> 1812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[0;32m   1813\u001b[0m         msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If the repo is private or gated, make sure to log in with `huggingface-cli login`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1814\u001b[0m     )\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: Dataset 'dataset/my-own-dataset' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "dataset = load_dataset(\"dataset/my-own-dataset\") #needed to construct a new Dataset class, to be done\n",
    "num_of_labels = ...\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"mshamrai/bert-base-ukr-eng-rus-uncased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"mshamrai/bert-base-ukr-eng-rus-uncased\", num_labels=num_of_labels)  # #number of labels\n",
    "\n",
    "\n",
    "label_list = ...  #Collect all the labels like 'O', 'B-inscription', 'I-inscription'\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = examples[\"ner_tags\"]\n",
    "    aligned_labels = []\n",
    "    for i, label_list in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_label = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            \n",
    "            if word_idx is None:\n",
    "                aligned_label.append(-100)\n",
    "            \n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_label.append(label_list[word_idx])\n",
    "            \n",
    "            else:\n",
    "                aligned_label.append(label_list[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels.append(aligned_label)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "val_dataset = tokenized_dataset[\"validation\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfd536-4e33-4e0e-9620-34234af2d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= ...,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir= ..,\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model= ...,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "    true_labels = [[label for label in example if label != -100] for example in labels]\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, label) in zip(pred, true_labels) if label != -100] for pred, true_labels in zip(predictions, true_labels)\n",
    "    ]\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    report = classification_report(true_labels, true_predictions, labels=label_list)\n",
    "    print(report)\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model('model/bert-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641b8f6-c1d3-4748-9e20-2e84aa4191fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
