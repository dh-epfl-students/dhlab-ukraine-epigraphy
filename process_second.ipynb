{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28942f6-7d74-464b-baed-9a332b5a6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Test_second/epigraphic_corpus-2.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1aa8e14-a408-41ee-a1be-c9a5fbe4508b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<doc id=\"file32708451\" filename=\"Epigrafika\" parent_folder=\"web_wikipedia\" url=\"https://uk.wikipedia.org/wiki/%D0%95%D0%BF%D1%96%D0%B3%D1%80%D0%B0%D1%84%D1%96%D0%BA%D0%B0\">\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6303111d-63e6-42a0-bdba-8134186367b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file has been seperated to 20 parts, and saved in data/Test_second.\n"
     ]
    }
   ],
   "source": [
    "def split_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    total_lines = len(lines)\n",
    "    \n",
    "    lines_per_part = total_lines // 40\n",
    "\n",
    "    remainder = total_lines % 40\n",
    "    \n",
    "    output_dir = \"data/Test_second\"\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    start = 0\n",
    "    for i in range(40):\n",
    "        if i < remainder:\n",
    "            lines_in_part = lines_per_part + 1\n",
    "        else:\n",
    "            lines_in_part = lines_per_part\n",
    "        \n",
    "        end = start + lines_in_part\n",
    "        part_lines = lines[start:end]\n",
    "\n",
    "        part_file_path = os.path.join(output_dir, f\"part_{i + 1}.txt\")\n",
    "        with open(part_file_path, 'w', encoding='utf-8') as part_file:\n",
    "            part_file.writelines(part_lines)\n",
    "\n",
    "        start = end\n",
    "    \n",
    "    print(f\"Original file has been seperated to 40 parts, and saved in {output_dir}.\")\n",
    "\n",
    "# 使用示例\n",
    "split_txt_file('data/Test_second/epigraphic_corpus-2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406a5ac9-5594-4bf1-9596-b219303fd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "model_path = 'model/first_model'\n",
    "nlp = spacy.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "603cf32e-c046-4cdc-abde-ea95aa17cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [21:58<00:00, 32.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('uk_core_news_lg')\n",
    "\n",
    "# Assume 'annotations' DataFrame is loaded and combined\n",
    "# Tokenize keywords in advance\n",
    "annotation_path = 'data/annotation/annotation.csv'\n",
    "annotations = pd.read_csv(annotation_path)\n",
    "annotations['keyword_tokens'] = annotations['keywords'].apply(lambda kw: [token.text for token in nlp(kw)])\n",
    "\n",
    "# List of file paths\n",
    "files = [\n",
    "    \"data/Test_second/part_1.txt\",\n",
    "    \"data/Test_second/part_2.txt\",\n",
    "    \"data/Test_second/part_3.txt\",\n",
    "    \"data/Test_second/part_4.txt\",\n",
    "    \"data/Test_second/part_5.txt\",\n",
    "    \"data/Test_second/part_6.txt\",\n",
    "    \"data/Test_second/part_7.txt\",\n",
    "    \"data/Test_second/part_8.txt\",\n",
    "    \"data/Test_second/part_9.txt\",\n",
    "    \"data/Test_second/part_10.txt\",\n",
    "    \"data/Test_second/part_11.txt\",\n",
    "    \"data/Test_second/part_12.txt\",\n",
    "    \"data/Test_second/part_13.txt\",\n",
    "    \"data/Test_second/part_14.txt\",\n",
    "    \"data/Test_second/part_15.txt\",\n",
    "    \"data/Test_second/part_16.txt\",\n",
    "    \"data/Test_second/part_17.txt\",\n",
    "    \"data/Test_second/part_18.txt\",\n",
    "    \"data/Test_second/part_19.txt\",\n",
    "    \"data/Test_second/part_20.txt\",\n",
    "    \"data/Test_second/part_21.txt\",\n",
    "    \"data/Test_second/part_22.txt\",\n",
    "    \"data/Test_second/part_23.txt\",\n",
    "    \"data/Test_second/part_24.txt\",\n",
    "    \"data/Test_second/part_25.txt\",\n",
    "    \"data/Test_second/part_26.txt\",\n",
    "    \"data/Test_second/part_27.txt\",\n",
    "    \"data/Test_second/part_28.txt\",\n",
    "    \"data/Test_second/part_29.txt\",\n",
    "    \"data/Test_second/part_30.txt\",\n",
    "    \"data/Test_second/part_31.txt\",\n",
    "    \"data/Test_second/part_32.txt\",\n",
    "    \"data/Test_second/part_33.txt\",\n",
    "    \"data/Test_second/part_34.txt\",\n",
    "    \"data/Test_second/part_35.txt\",\n",
    "    \"data/Test_second/part_36.txt\",\n",
    "    \"data/Test_second/part_37.txt\",\n",
    "    \"data/Test_second/part_38.txt\",\n",
    "    \"data/Test_second/part_39.txt\",\n",
    "    \"data/Test_second/part_40.txt\",\n",
    "    # Assume other files as needed\n",
    "]\n",
    "\n",
    "# Prepare to write to the TSV file\n",
    "with open(\"data/Test_second/data/data.tsv\", \"w\", encoding='utf-8') as f:\n",
    "    # Process each file\n",
    "    for file_path in tqdm(files, total=len(files)):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        doc = nlp(text.strip())\n",
    "\n",
    "        # Iterate over the sentences\n",
    "        for sent in doc.sents:\n",
    "            sent_tokens = [token.text for token in sent if token.text not in ['\\t', '\\n', ' ']]\n",
    "            token_labels = ['O' for _ in sent_tokens]  # Initialize labels as 'O' for each token\n",
    "\n",
    "            # Check for matching keyword token sequences\n",
    "            for _, row in annotations.iterrows():\n",
    "                keyword_tokens = row['keyword_tokens']\n",
    "                for i in range(len(sent_tokens) - len(keyword_tokens) + 1):\n",
    "                    if sent_tokens[i:i + len(keyword_tokens)] == keyword_tokens:\n",
    "                        # Match found, apply BIO tagging\n",
    "                        if all(label == 'O' for label in token_labels[i:i+len(keyword_tokens)]):  # Ensure not to overwrite existing tags\n",
    "                            token_labels[i] = f'B-{row[\"type\"]}'\n",
    "                            for j in range(1, len(keyword_tokens)):\n",
    "                                token_labels[i + j] = f'I-{row[\"type\"]}'\n",
    "\n",
    "            # Write tokens and tags to file, with newline after each sentence\n",
    "            for token, label in zip(sent_tokens, token_labels):\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")  # Newline after each sentence to separate them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5f25d-2cee-4fef-baf2-47bfc485bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
